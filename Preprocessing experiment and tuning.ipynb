{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "from scipy.fftpack import dct\n",
    "import os \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import wave\n",
    "import math\n",
    "import scipy.io.wavfile as wav\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For preprocessing experiment and model tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data_dir):\n",
    "    real_files = [os.path.join(data_dir, \"real\", f) for f in os.listdir(os.path.join(data_dir, \"real\")) if f.endswith(\".wav\")]\n",
    "    fake_files = [os.path.join(data_dir, \"fake\", f) for f in os.listdir(os.path.join(data_dir, \"fake\")) if f.endswith(\".wav\")]\n",
    "    fake_labels = [0] * len(fake_files)\n",
    "    real_labels = [1] * len(real_files)\n",
    "\n",
    "    files = fake_files + real_files\n",
    "    labels = fake_labels + real_labels\n",
    "\n",
    "    return files, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_data(path, processed_data, labels):\n",
    "    data_dict = {\n",
    "        'data': [],\n",
    "        'label': labels\n",
    "    }\n",
    "    for i in range(len(processed_data)):\n",
    "        data_dict['data'].append(processed_data[i].tolist())\n",
    "    with open(path, 'w') as file:\n",
    "        json.dump(data_dict, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def preprocessing(audio_file, n_features):\n",
    "    # Load the audio signal\n",
    "    y, sr = librosa.load(audio_file, sr=None)\n",
    "\n",
    "    # Step 1: Pre-emphasis\n",
    "    pre_emphasis_coeff = 0.97\n",
    "    y_filt = librosa.effects.preemphasis(y, coef=pre_emphasis_coeff)\n",
    "\n",
    "    # Step 2: Frame blocking\n",
    "    frame_length = 0.025*4  # 25 ms\n",
    "    hop_length = 0.01  # 10 ms\n",
    "    frame_length_samples = int(frame_length * sr)\n",
    "    hop_length_samples = int(hop_length * sr)\n",
    "    frames = librosa.util.frame(y_filt, frame_length=frame_length_samples, hop_length=hop_length_samples)\n",
    "\n",
    "    # Step 3: Windowing\n",
    "    window = np.hamming(len(frames))\n",
    "    windowed_frames = frames * window[:, np.newaxis]\n",
    "    \n",
    "    # Step 4: Fast Fourier Transform (FFT)\n",
    "    fft_size = 2048\n",
    "    spectrogram = np.abs(np.fft.rfft(windowed_frames, n=fft_size, axis=0))\n",
    "\n",
    "    # Step 5: Mel frequency wrapping\n",
    "    if n_features >= 40:\n",
    "        n_mels = n_features\n",
    "    else:\n",
    "        n_mels = 40  # Adjusted number of Mel bands\n",
    "    mel_spec = librosa.feature.melspectrogram(S=spectrogram, sr=sr, n_mels=n_mels)\n",
    "    \n",
    "    # Step 6: Discrete Cosine Transform (DCT) to get MFCC\n",
    "    n_mfcc = n_features  # Use the desired number of MFCC features\n",
    "    mfcc = librosa.feature.mfcc(S=librosa.power_to_db(mel_spec), sr=sr, n_mfcc=n_mfcc)\n",
    "    \n",
    "    mean_mfcc = np.max(mfcc, axis=1)\n",
    "\n",
    "    # Initialize the StandardScaler\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    # Reshape the aggregated MFCCs to 2D (needed for fitting the scaler)\n",
    "    mean_mfcc_reshaped = mean_mfcc.reshape(-1, 1)\n",
    "\n",
    "    # Fit and transform the data\n",
    "    mean_mfcc_scaled = scaler.fit_transform(mean_mfcc_reshaped)\n",
    "\n",
    "    # Flatten the scaled data back to 1D\n",
    "    mean_mfcc_scaled_flat = mean_mfcc_scaled.flatten()\n",
    "\n",
    "    return mean_mfcc_scaled_flat\n",
    "#code 9x accu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data_dir):\n",
    "    real_files = [os.path.join(data_dir, \"real\", f) for f in os.listdir(os.path.join(data_dir, \"real\")) if f.endswith(\".wav\")]\n",
    "    fake_files = [os.path.join(data_dir, \"fake\", f) for f in os.listdir(os.path.join(data_dir, \"fake\")) if f.endswith(\".wav\")]\n",
    "    real_files1 = [os.path.join(data_dir, \"real_an\", f) for f in os.listdir(os.path.join(data_dir, \"real_an\")) if f.endswith(\".wav\")][:800]\n",
    "    fake_files1 = [os.path.join(data_dir, \"fake_an\", f) for f in os.listdir(os.path.join(data_dir, \"fake_an\")) if f.endswith(\".wav\")]\n",
    "\n",
    "    real_files = real_files+real_files1\n",
    "    fake_files = fake_files+fake_files1\n",
    "    fake_labels = [0] * len(fake_files)\n",
    "    real_labels = [1] * len(real_files)\n",
    "\n",
    "    files = fake_files + real_files\n",
    "    labels = fake_labels + real_labels\n",
    "    print(len(fake_files))\n",
    "    print(len(real_files))\n",
    "    return files, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2511\n",
      "2772\n"
     ]
    }
   ],
   "source": [
    "url = r\"C:\\Users\\VIET HOANG - VTS\\Desktop\\tien xu ly\"\n",
    "files, labels = load_data(url)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(files, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "X_train = [preprocessing(file,68) for file in X_train]\n",
    "X_test = [preprocessing(file,68) for file in X_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression: Accuracy = 0.8836329233680227\n",
      "DecisionTreeClassifier: Accuracy = 0.9082308420056765\n",
      "SVC: Accuracy = 0.9943235572374646\n",
      "RandomForestClassifier: Accuracy = 0.97918637653737\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\VIET HOANG - VTS\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoostClassifier: Accuracy = 0.9139072847682119\n",
      "KNeighborsClassifier: Accuracy = 0.9914853358561968\n",
      "GaussianNB: Accuracy = 0.8164616840113529\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "# Create a list of classifiers\n",
    "classifiers = [\n",
    "    LogisticRegression(),\n",
    "    DecisionTreeClassifier(),\n",
    "    SVC(C=2,gamma=3),\n",
    "    RandomForestClassifier(),\n",
    "    AdaBoostClassifier(),\n",
    "    KNeighborsClassifier(),\n",
    "    GaussianNB()\n",
    "]\n",
    "\n",
    "# Train and evaluate each classifier\n",
    "for classifier in classifiers:\n",
    "    classifier.fit(X_train, y_train)\n",
    "    y_pred = classifier.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"{classifier.__class__.__name__}: Accuracy = {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1086\n"
     ]
    }
   ],
   "source": [
    "print(len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9943235572374646\n"
     ]
    }
   ],
   "source": [
    "svmm = SVC(C=2,gamma=3)\n",
    "svmm.fit(X_train,y_train)\n",
    "print(svmm.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9772942289498581"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf = RandomForestClassifier(min_samples_split=2)\n",
    "rf.fit(X_train,y_train)\n",
    "rf.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train Tr√†, Test An"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_an(data_dir):\n",
    "    real_files = [os.path.join(data_dir, \"real_an\", f) for f in os.listdir(os.path.join(data_dir, \"real_an\")) if f.endswith(\".wav\")][:800]\n",
    "    fake_files = [os.path.join(data_dir, \"fake_an\", f) for f in os.listdir(os.path.join(data_dir, \"fake_an\")) if f.endswith(\".wav\")]\n",
    "\n",
    "    fake_labels = [0] * len(fake_files)\n",
    "    real_labels = [1] * len(real_files)\n",
    "    print(len(real_files))\n",
    "    print(len(fake_files))\n",
    "    files = fake_files + real_files\n",
    "    labels = fake_labels + real_labels\n",
    "    return files, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800\n",
      "539\n"
     ]
    }
   ],
   "source": [
    "url = r\"C:\\Users\\VIET HOANG - VTS\\Desktop\\tien xu ly\"\n",
    "files, labels = load_data_an(url)\n",
    "\n",
    "\n",
    "X_test = [preprocessing(file,68) for file in files]\n",
    "y_test = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_path = r\"C:\\Users\\VIET HOANG - VTS\\Desktop\\tien xu ly\\processed_data\\test_an.json\"\n",
    "save_data(process_path, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_path = r\"C:\\Users\\VIET HOANG - VTS\\Desktop\\tien xu ly\\processed_data\\test_an_dct.json\"\n",
    "save_data(process_path, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_tra(data_dir):\n",
    "    real_files = [os.path.join(data_dir, \"real\", f) for f in os.listdir(os.path.join(data_dir, \"real\")) if f.endswith(\".wav\")]\n",
    "    fake_files = [os.path.join(data_dir, \"fake\", f) for f in os.listdir(os.path.join(data_dir, \"fake\")) if f.endswith(\".wav\")]\n",
    "\n",
    "    fake_labels = [0] * len(fake_files)\n",
    "    real_labels = [1] * len(real_files)\n",
    "\n",
    "    files = fake_files + real_files\n",
    "    labels = fake_labels + real_labels\n",
    "    return files, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = r\"C:\\Users\\VIET HOANG - VTS\\Desktop\\tien xu ly\"\n",
    "files, labels = load_data_tra(url)\n",
    "\n",
    "\n",
    "X_train = [preprocessing(file,68) for file in files]\n",
    "y_train = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_path = r\"C:\\Users\\VIET HOANG - VTS\\Desktop\\tien xu ly\\processed_data\\train_tra.json\"\n",
    "save_data(process_path, X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_path = r\"C:\\Users\\VIET HOANG - VTS\\Desktop\\tien xu ly\\processed_data\\train_tra_dct.json\"\n",
    "save_data(process_path, X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression: Accuracy = 0.5235250186706497\n",
      "DecisionTreeClassifier: Accuracy = 0.5743091859596714\n",
      "SVC: Accuracy = 0.6706497386109037\n",
      "RandomForestClassifier: Accuracy = 0.5967139656460044\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\VIET HOANG - VTS\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoostClassifier: Accuracy = 0.5832710978342046\n",
      "KNeighborsClassifier: Accuracy = 0.666168782673637\n",
      "GaussianNB: Accuracy = 0.5242718446601942\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "# Create a list of classifiers\n",
    "classifiers = [\n",
    "    LogisticRegression(),\n",
    "    DecisionTreeClassifier(),\n",
    "    SVC(gamma=1),\n",
    "    RandomForestClassifier(),\n",
    "    AdaBoostClassifier(),\n",
    "    KNeighborsClassifier(n_neighbors=2,weights='uniform'),\n",
    "    GaussianNB()\n",
    "]\n",
    "\n",
    "# Train and evaluate each classifier\n",
    "for classifier in classifiers:\n",
    "    classifier.fit(X_train, y_train)\n",
    "    y_pred = classifier.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"{classifier.__class__.__name__}: Accuracy = {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3944\n",
      "1339\n",
      "0.3395030425963489\n"
     ]
    }
   ],
   "source": [
    "print(len(X_train))\n",
    "print(len(X_test))\n",
    "print(len(X_test)/len(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.648991784914115"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svmm  =SVC(C=13,gamma=4,probability=True)\n",
    "svmm.fit(X_train,y_train)\n",
    "svmm.score(X_test,y_test)\n",
    "\n",
    "#doi voi DCT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_an = r\"C:\\Users\\VIET HOANG - VTS\\Desktop\\tien xu ly\\processed_data\\test_an.json\" # to load data an\n",
    "process_tra = r\"C:\\Users\\VIET HOANG - VTS\\Desktop\\tien xu ly\\processed_data\\train_tra.json\" # to load data tra\n",
    "X_test, y_test = load_data(process_an)\n",
    "X_train, y_train = load_data(process_tra)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1,300):\n",
    "    lg = SVC(C=i,gamma=1)\n",
    "    lg.fit(X_train,y_train)\n",
    "    print(f\"{i} : {lg.score(X_test,y_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(100,300,5):\n",
    "    lg = RandomForestClassifier(n_estimators=i)\n",
    "    lg.fit(X_train,y_train)\n",
    "    \n",
    "    print(f\"{i} : {lg.score(X_test,y_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1,300):\n",
    "    lg = KNeighborsClassifier(n_neighbors=i,weights='uniform')\n",
    "    lg.fit(X_train,y_train)\n",
    "    \n",
    "    print(f\"{i} : {lg.score(X_test,y_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7326362957430919"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svmm  =SVC(C=30,gamma=1,probability=True)\n",
    "knb = KNeighborsClassifier(n_neighbors=1,weights='uniform')\n",
    "voting = VotingClassifier(\n",
    "    estimators=[('svm', svmm),('rf',knb)],\n",
    "    voting='soft'\n",
    ")\n",
    "voting.fit(X_train,y_train)\n",
    "voting.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8681022880215343\n"
     ]
    }
   ],
   "source": [
    "lg = LogisticRegression(C=0.09,solver='liblinear')\n",
    "lg.fit(X_train,y_train)\n",
    "print(lg.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "process_path = r\"C:\\Users\\VIET HOANG - VTS\\Desktop\\tien xu ly\\processed_data\\frameblock100ms.json\"\n",
    "def save_data(path, processed_data, labels):\n",
    "    data_dict = {\n",
    "        'data': [],\n",
    "        'label': labels\n",
    "    }\n",
    "    for i in range(len(processed_data)):\n",
    "        data_dict['data'].append(processed_data[i].tolist())\n",
    "    with open(path, 'w') as file:\n",
    "        json.dump(data_dict, file)\n",
    "\n",
    "processed_data = X_train+X_test\n",
    "labels = y_train+y_test\n",
    "save_data(process_path, processed_data, labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1339\n"
     ]
    }
   ],
   "source": [
    "def load_data(path):\n",
    "    with open(path, 'r') as file:\n",
    "        data_dict = json.load(file)\n",
    "    processed_data = data_dict['data']\n",
    "    labels = data_dict['label']\n",
    "    return processed_data, labels\n",
    "\n",
    "# Example usage:\n",
    "processed_data, labels = load_data(process_path)\n",
    "print(len(processed_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, path):\n",
    "    with open(path, 'wb') as file:\n",
    "        pickle.dump(model, file)\n",
    "model_path = r\"C:\\Users\\VIET HOANG - VTS\\Desktop\\tien xu ly\\model\\model_frame100ms.pkl\"\n",
    "save_model(voting,model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "def load_model(path):\n",
    "    with open(path, 'rb') as file:\n",
    "        model = pickle.load(file)\n",
    "    return model\n",
    "loaded_model = load_model(model_path)\n",
    "print(loaded_model.predict(np.array(processed_data[0]).reshape(1,-1)))\n",
    "print(y_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data_dir):\n",
    "    real_files = [os.path.join(data_dir, \"real\", f) for f in os.listdir(os.path.join(data_dir, \"real\")) if f.endswith(\".wav\")]\n",
    "    fake_files = [os.path.join(data_dir, \"fake\", f) for f in os.listdir(os.path.join(data_dir, \"fake\")) if f.endswith(\".wav\")]\n",
    "    real_files1 = [os.path.join(data_dir, \"real_an\", f) for f in os.listdir(os.path.join(data_dir, \"real_an\")) if f.endswith(\".wav\")][:800]\n",
    "    fake_files1 = [os.path.join(data_dir, \"fake_an\", f) for f in os.listdir(os.path.join(data_dir, \"fake_an\")) if f.endswith(\".wav\")]\n",
    "\n",
    "    real_files = real_files+real_files1\n",
    "    fake_files = fake_files+fake_files1\n",
    "    fake_labels = [0] * len(fake_files)\n",
    "    real_labels = [1] * len(real_files)\n",
    "\n",
    "    files = fake_files + real_files\n",
    "    labels = fake_labels + real_labels\n",
    "    print(len(fake_files))\n",
    "    print(len(real_files))\n",
    "    return files, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_data(path, processed_data, labels):\n",
    "    data_dict = {\n",
    "        'data': [],\n",
    "        'label': labels\n",
    "    }\n",
    "    for i in range(len(processed_data)):\n",
    "        data_dict['data'].append(processed_data[i].tolist())\n",
    "    with open(path, 'w') as file:\n",
    "        json.dump(data_dict, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from librosa import ParameterError\n",
    "def detect(audio_file):\n",
    "    try:\n",
    "        # Load the audio signal\n",
    "        y, sr = librosa.load(audio_file, sr=None)\n",
    "\n",
    "        # Step 1: Pre-emphasis\n",
    "        pre_emphasis_coeff = 0.97\n",
    "        y_filt = librosa.effects.preemphasis(y, coef=pre_emphasis_coeff)\n",
    "\n",
    "        # Step 2: Frame blocking\n",
    "        frame_length = 0.025*4  # 25 ms\n",
    "        hop_length = 0.01  # 10 ms\n",
    "        frame_length_samples = int(frame_length * sr)\n",
    "        hop_length_samples = int(hop_length * sr)\n",
    "        frames = librosa.util.frame(y_filt, frame_length=frame_length_samples, hop_length=hop_length_samples)\n",
    "    except ParameterError:\n",
    "        print(f\"ParameterError encountered with file {audio_file}. Deleting file.\")\n",
    "        os.remove(audio_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_ult(data_dir):\n",
    "    real_files = [os.path.join(data_dir, \"real\", f) for f in os.listdir(os.path.join(data_dir, \"real\")) if f.endswith(\".wav\")]\n",
    "    fake_files = [os.path.join(data_dir, \"fake\", f) for f in os.listdir(os.path.join(data_dir, \"fake\")) if f.endswith(\".wav\")]\n",
    "\n",
    "    real_files1 = [os.path.join(data_dir, \"real_an\", f) for f in os.listdir(os.path.join(data_dir, \"real_an\")) if f.endswith(\".wav\")]\n",
    "    fake_files1 = [os.path.join(data_dir, \"fake_an\", f) for f in os.listdir(os.path.join(data_dir, \"fake_an\")) if f.endswith(\".wav\")]\n",
    "    \n",
    "    real_files2 = [os.path.join(data_dir, \"predict_real\", f) for f in os.listdir(os.path.join(data_dir, \"predict_real\")) if f.endswith(\".wav\")]\n",
    "    fake_files2 = [os.path.join(data_dir, \"predict_fake\", f) for f in os.listdir(os.path.join(data_dir, \"predict_fake\")) if f.endswith(\".wav\")]\n",
    "    \n",
    "    real_files = real_files+real_files1+real_files2\n",
    "    fake_files = fake_files+fake_files1+fake_files2\n",
    "    fake_labels = [0] * len(fake_files)\n",
    "    real_labels = [1] * len(real_files)\n",
    "\n",
    "    files = fake_files + real_files\n",
    "    labels = fake_labels + real_labels\n",
    "    print(len(fake_files))\n",
    "    print(len(real_files))\n",
    "    return files, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_fake(data_dir):\n",
    "    fake_files = [os.path.join(data_dir, \"fake_shit\", f) for f in os.listdir(os.path.join(data_dir, \"fake_shit\")) if f.endswith(\".wav\")]\n",
    "    \n",
    "\n",
    "    fake_labels = [0] * len(fake_files)\n",
    "\n",
    "    return fake_files,fake_labels \n",
    "url = r\"C:\\Users\\VIET HOANG - VTS\\Desktop\\tien xu ly\" \n",
    "files, labels = load_fake(url)\n",
    "\n",
    "files = [preprocessing(file,68) for file in files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_data(path, processed_data, labels):\n",
    "    data_dict = {\n",
    "        'data': [],\n",
    "        'label': labels\n",
    "    }\n",
    "    for i in range(len(processed_data)):\n",
    "        data_dict['data'].append(processed_data[i].tolist())\n",
    "    with open(path, 'w') as file:\n",
    "        json.dump(data_dict, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_ult(data_dir):\n",
    "    real_files = [os.path.join(data_dir, \"real\", f) for f in os.listdir(os.path.join(data_dir, \"real\")) if f.endswith(\".wav\")]\n",
    "    fake_files = [os.path.join(data_dir, \"fake\", f) for f in os.listdir(os.path.join(data_dir, \"fake\")) if f.endswith(\".wav\")]\n",
    "\n",
    "    real_files1 = [os.path.join(data_dir, \"real_an\", f) for f in os.listdir(os.path.join(data_dir, \"real_an\")) if f.endswith(\".wav\")]\n",
    "    fake_files1 = [os.path.join(data_dir, \"fake_an\", f) for f in os.listdir(os.path.join(data_dir, \"fake_an\")) if f.endswith(\".wav\")]\n",
    "    \n",
    "    real_files2 = [os.path.join(data_dir, \"predict_real\", f) for f in os.listdir(os.path.join(data_dir, \"predict_real\")) if f.endswith(\".wav\")]\n",
    "    fake_files2 = [os.path.join(data_dir, \"predict_fake\", f) for f in os.listdir(os.path.join(data_dir, \"predict_fake\")) if f.endswith(\".wav\")]\n",
    "    \n",
    "    real_files = real_files+real_files1+real_files2\n",
    "    fake_files = fake_files+fake_files1+fake_files2\n",
    "    fake_labels = [0] * len(fake_files)\n",
    "    real_labels = [1] * len(real_files)\n",
    "\n",
    "    files = fake_files + real_files\n",
    "    labels = fake_labels + real_labels\n",
    "    print(len(fake_files))\n",
    "    print(len(real_files))\n",
    "    return files, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run and save mfcc features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(20,81):\n",
    "    url = r\"C:\\Users\\VIET HOANG - VTS\\Desktop\\tien xu ly\"\n",
    "    files, labels = load_data_ult(url)\n",
    "    files = [preprocessing(file,i) for file in files]\n",
    "    url = f\"C:\\\\Users\\\\VIET HOANG - VTS\\\\Desktop\\\\tien xu ly\\\\processed\\\\dct_{i}.json\"\n",
    "    save_data(url,files,labels)\n",
    "    print(f\"Save data succeed: {url}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_fake(data_dir):\n",
    "    fake_files = [os.path.join(data_dir, \"fake_shit\", f) for f in os.listdir(os.path.join(data_dir, \"fake_shit\")) if f.endswith(\".wav\")]\n",
    "    \n",
    "\n",
    "    fake_labels = [0] * len(fake_files)\n",
    "\n",
    "    return fake_files,fake_labels \n",
    "url = r\"C:\\Users\\VIET HOANG - VTS\\Desktop\\tien xu ly\" \n",
    "files, labels = load_fake(url)\n",
    "\n",
    "files = [detect(file) for file in files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_data(path, processed_data, labels):\n",
    "    data_dict = {\n",
    "        'data': [],\n",
    "        'label': labels\n",
    "    }\n",
    "    for i in range(len(processed_data)):\n",
    "        data_dict['data'].append(processed_data[i].tolist())\n",
    "    with open(path, 'w') as file:\n",
    "        json.dump(data_dict, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path):\n",
    "    with open(path, 'r') as file:\n",
    "        data_dict = json.load(file)\n",
    "    processed_data = data_dict['data']\n",
    "    labels = data_dict['label']\n",
    "    return processed_data, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merg(path1,path2):\n",
    "    with open(path1, 'r') as file:\n",
    "        data_1 = json.load(file)\n",
    "    with open(path2, 'r') as file:\n",
    "        data_2 = json.load(file)\n",
    "    processed_data = data_1['data']+data_2['data']\n",
    "    labels = data_1['label']+data_2['label']\n",
    "    data_dict = {\n",
    "        'data': processed_data,\n",
    "        'label': labels\n",
    "    }\n",
    "    with open(path1, 'w') as file:\n",
    "        json.dump(data_dict, file)\n",
    "for i in range(20,81):\n",
    "    path1 = f\"C:\\\\Users\\\\VIET HOANG - VTS\\\\Desktop\\\\tien xu ly\\\\data_ne\\\\dct_{i}.json\"\n",
    "    path2 = f\"C:\\\\Users\\\\VIET HOANG - VTS\\\\Desktop\\\\tien xu ly\\\\processed_fake\\\\fake_{i}.json\"\n",
    "    merg(path1,path2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
